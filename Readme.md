
# MACHINE-LEARNING-MODELS

This repository contains various machine learning models and notebooks showcasing implementations and experiments on different datasets. The primary focus is on the housing dataset, but other models and techniques are included as well.

## Repository Structure
### Linear Regression

The Linear-Regression Model folder contains an advanced machine learning approach applied to a housing dataset. This includes:

- Data preprocessing: Handling missing values, encoding categorical variables, and scaling features.
- Feature engineering: Creating new features from existing ones.
- Model training: Training a linear regression model on the preprocessed data.
- Model evaluation: Evaluating the model's performance using metrics like Mean Squared Error (MSE) and RÂ² score.

### Logistic Regression

The Logistic Regression Model folder includes examples and applications of logistic regression, demonstrating binary classification tasks such as predicting the probability of an event occurring.

### University of Washington-ML Specialization

This section includes notes and exercises from the University of Washington's Machine Learning Specialization by Emily Fox & Carlos Guestrin. Topics covered include:

- Recommender systems: Collaborative filtering, matrix factorization, and content-based recommendations.
- Feature scaling: Normalizing and standardizing features for better model performance.
- Learning rates: Exploring the impact of different learning rates on the convergence of gradient descent.

### Feature Scaling and Learning Rate

The `Feature_Scaling_and_Learning_Rate.ipynb` notebook demonstrates:

- The importance of feature scaling in machine learning: Techniques like Min-Max Scaling and Standardization.
- How different learning rates affect model convergence and performance: Visualizations and experiments with various learning rates.

### Gradient Descent using Scikit-learn

The `Sklearn_GD.ipynb` notebook provides an example of implementing gradient descent using scikit-learn, covering:

- Linear regression using gradient descent.
- Visualization of the cost function and convergence of the algorithm.

## Getting Started

### Prerequisites

Make sure you have the following installed:

- Python 3.x
- Jupyter Notebook
- Scikit-learn
- Pandas
- Numpy
- Matplotlib

You can install the required Python packages using pip:

```bash
pip install scikit-learn pandas numpy matplotlib
```

### Running the Notebooks

To run the notebooks, follow these steps:

1. **Clone the repository:**

   ```bash
   git clone https://github.com/addygeek/MACHINE-LEARNING-MODELS.git
   cd MACHINE-LEARNING-MODELS
   ```

2. **Start Jupyter Notebook:**

   ```bash
   jupyter notebook
   ```

3. **Open the desired notebook:**
   In the Jupyter Notebook interface, navigate to the folder containing the notebook you want to explore and click on it to open.

4. **Run the cells:**
   Run the cells sequentially by clicking on the "Run" button or by pressing `Shift + Enter`.


## Contributing

Contributions are welcome! Please create a pull request or open an issue to discuss your ideas.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- University of Washington's Machine Learning Specialization by Emily Fox & Carlos Guestrin
- Scikit-learn documentation and tutorials

## Repository Link

For more details, visit the [MACHINE-LEARNING-MODELS repository](https://github.com/addygeek/MACHINE-LEARNING-MODELS).
